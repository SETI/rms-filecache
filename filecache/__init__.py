"""The purpose of the `filecache` module is to abstract away the location where files
used or generated by a program are stored. Files can be on the local file system, in
Google Cloud Storage, on Amazon Web Services S3, or on a webserver. When files to be read
are on the local file system, they are simply accessed in-place. Otherwise, they are
downloaded from the remote source to a local temporary directory. When files to be written
are on the local file system, they are simply written in-place. Otherwise, they are
written to a local temporary directory and then uploaded to the remote location (it is not
possible to upload to a webserver). When a cache is no longer needed, it is deleted from
the local disk.

The top-level file organization is provided by the :class:`FileCache` class. A
:class:`FileCache` instance is used to specify a particular **sharing policy** and
**lifetime**. For example, a cache could be private to the current process and group a set
of files that all have the same basic purpose. Once these files have been (downloaded and)
read, they are deleted as a group. Another cache could be shared among all processes on
the current machine and group a set of files that are needed by multiple processes, thus
allowing them to be downloaded from a remote source only one time, saving time and
bandwidth.

A :class:`FileCache` can be instantiated either directly or as a context manager. When
instantiated directly, the programmer is responsible for calling
:meth:`FileCache.clean_up` directly to delete the cache when finished. In addition, a
non-shared cache will be deleted on program exit. When instantiated as a context manager,
a non-shared cache is deleted on exit from the context. See the class documentation for
full details.

Usage examples::

    from filecache import FileCache
    with FileCache() as fc:  # Use as context manager
        # Also use open() as a context manager
        with fc.open('gs://rms-filecache-tests/subdir1/subdir2a/binary1.bin', 'rb',
                     anonymous=True) as fp:
            bin1 = fp.read()
        with fc.open('s3://rms-filecache-tests/subdir1/subdir2a/binary1.bin', 'rb',
                     anonymous=True) as fp:
            bin2 = fp.read()
        assert bin1 == bin2
    # Cache automatically deleted here

    fc = FileCache()  # Use without context manager
    # Also retrieve file without using open context manager
    path1 = fc.retrieve('gs://rms-filecache-tests/subdir1/subdir2a/binary1.bin',
                        anonymous=True)
    with open(path1, 'rb') as fp:
        bin1 = fp.read()
    path2 = fc.retrieve('s3://rms-filecache-tests/subdir1/subdir2a/binary1.bin',
                        anonymous=True)
    with open(path2, 'rb') as fp:
        bin2 = fp.read()
    fc.clean_up()  # Cache manually deleted here
    assert bin1 == bin2

    # Write a file to a bucket and read it back
    with FileCache() as fc:
        with fc.open('gs://my-writable-bucket/output.txt', 'w') as fp:
            fp.write('A')
    # The cache will be deleted here so the file will have to be downloaded
    with FileCache() as fc:
        with fc.open('gs://my-writable-bucket/output.txt', 'r') as fp:
            print(fp.read())

A :class:`FileCachePrefix` instance can be used to encapsulate the storage prefix string,
as well as any subdirectories, plus various arguments such as `anonymous` and `time_out`
that can be specified to each `exists`, `retrieve`, or `upload` method. Thus using one
of these instances can simplify the use of a :class:`FileCache` by allowing the user to
only specify the relative part of the path to be operated on, and to not specify various
other parameters at each method call site.

Compare this example to the one above::

    from filecache import FileCache
    with FileCache() as fc:  # Use as context manager
        # Use GS by specifying the bucket name and one directory level
        pfx1 = fc.new_prefix('gs://rms-filecache-tests/subdir1', anonymous=True)
        # Use S3 by specifying the bucket name and two directory levels
        pfx2 = fc.new_prefix('s3://rms-filecache-tests/subdir1/subdir2a', anonymous=True)
        # Access GS using a directory + filename (since only one directory level
        # was specified by the prefix)
        # Also use open() as a context manager
        with pfx1.open('subdir2a/binary1.bin', 'rb') as fp:
            bin1 = fp.read()
        # Access S3 using a filename only (since two directory levels were already
        # specified by the prefix))
        with pfx2.open('binary1.bin', 'rb') as fp:
            bin2 = fp.read()
        assert bin1 == bin2
    # Cache automatically deleted here

A benefit of the abstraction is that different environments can access the same files in
different ways without needing to change the program code. For example, consider a program
that needs to access the file ``COISS_2xxx/COISS_2001/voldesc.cat`` from the NASA PDS
archives. This file might be stored on the local disk in the user's home directory in a
subdirectory called ``pds3-holdings``. Or if the user does not have a local copy, it is
accessible from a webserver at
``https://pds-rings.seti.org/holdings/volumes/COISS_2xxx/COISS_2001/voldesc.cat``.
Finally, it could be accessible from Google Cloud Storage from the ``rms-node-holdings``
bucket at
``gs://rms-node-holdings/pds3-holdings/volumes/COISS_2xxx/COISS_2001/voldesc.cat``. Before
running the program, an environment variable could be set to one of these values::

    $ export PDS3_HOLDINGS_SRC="~/pds3-holdings"
    $ export PDS3_HOLDINGS_SRC="https://pds-rings.seti.org/holdings"
    $ export PDS3_HOLDINGS_SRC="gs://rms-node-holdings/pds3-holdings"

Then the program could be written as::

    from filecache import FileCache
    import os
    with FileCache() as fc:
        pfx = fc.new_prefix(os.getenv('PDS3_HOLDINGS_SRC'))
        with pfx.open('volumes/COISS_2xxx/COISS_2001/voldesc.cat', 'r') as fp:
            contents = fp.read()
    # Cache automatically deleted here

If the program was going to be run multiple times in a row, or multiple copies were going
to be run simultaneously, marking the cache as shared would allow all of the processes to
share the same copy, thus requiring only a single download no matter how many times the
program was run::

    from filecache import FileCache
    import os
    with FileCache(shared=True) as fc:
        pfx = fc.new_prefix(os.getenv('PDS3_HOLDINGS_DIR'))
        with pfx.open('volumes/COISS_2xxx/COISS_2001/voldesc.cat', 'r') as fp:
            contents = fp.read()
    # Cache not deleted here; must be deleted manually using fc.clean_up(final=True)
    # If not deleted manually, the shared cache will persist until the temporary
    # directory is purged by the operating system (which may be never)

Finally, there are four classes that allow direct access to the four possible storage
locations without invoking any caching behavior: :class:`FileCacheSourceLocal`,
:class:`FileCacheSourceHTTP`, :class:`FileCacheSourceGS`, and :class:`FileSourceCacheS3`::

    from filecache import FileCacheSourceGS
    src = FileCacheSourceGS('gs://rms-filecache-tests', anonymous=True)
    src.retrieve('subdir1/subdir2a/binary1.bin', 'local_file.bin')
"""

import atexit
import contextlib
import logging
from logging import Logger
import os
from pathlib import Path
import sys
import tempfile
import time
from typing import (cast,
                    Optional,
                    )
import uuid

import filelock

from .file_cache_source import (FileCacheSource,
                                FileCacheSourceLocal,
                                FileCacheSourceHTTP,
                                FileCacheSourceGS,
                                FileCacheSourceS3,
                                )
from .file_cache_prefix import FileCachePrefix

try:
    from ._version import __version__
except ImportError:  # pragma: no cover - only present when building a package
    __version__ = 'Version unspecified'


# Default logger for all FileCache instances that didn't specify one explicitly
_GLOBAL_LOGGER: Logger | None = None


# Global cache of all instantiated FileCacheSource since they may involve opening
# a connection and are not specific to a particular FileCache
_SOURCE_CACHE: dict[tuple[str, bool], FileCacheSource] = {}


def set_global_logger(logger):
    """Set the global logger for all FileCache instances that don't specify one."""
    global _GLOBAL_LOGGER
    logger = logger if logger else False  # Turn None into False
    _GLOBAL_LOGGER = logger


def set_easy_logger():
    """Set a default logger that outputs all messages to stdout."""
    easy_logger = logging.getLogger(__name__)
    easy_logger.setLevel(logging.DEBUG)

    while easy_logger.handlers:
        easy_logger.removeHandler(easy_logger.handlers[0])

    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    easy_logger.addHandler(handler)

    set_global_logger(easy_logger)


def get_global_logger():
    """Return the current global logger."""
    return _GLOBAL_LOGGER


class FileCache:
    """Class which manages the lifecycle of files from various sources."""

    _FILE_CACHE_PREFIX = '_filecache_'
    _LOCK_PREFIX = '.__lock__'

    def __init__(self, *,
                 cache_root: Optional[Path | str] = None,
                 cache_name: Optional[str] = None,
                 delete_on_exit: Optional[bool] = None,
                 mp_safe: Optional[bool] = None,
                 all_anonymous: bool = False,
                 lock_timeout: int = 60,
                 nthreads: int = 8,
                 logger: Optional[Logger | bool] = None):
        r"""Initialization for the FileCache class.

        When specifying the full path to a file, if the prefix starts with
        ``gs://bucket-name`` it is from Google Storage. If the prefix starts with
        ``s3://bucket-name`` it is from AWS S3. If the prefix starts with ``http://`` or
        ``https://`` it is from a website download. Anything else is considered to be in
        the local filesystem.

        Parameters:
            cache_root: The directory in which to cache files. By default, the system
                temporary directory is used, which involves checking the environment
                variables ``TMPDIR``, ``TEMP``, and ``TMP``, and if none of those are set
                then using ``C:\TEMP``, ``C:\TMP``, ``\TEMP``, or ``\TMP`` on Windows and
                ``/tmp``, ``/var/tmp``, or ``/usr/tmp`` on other platforms. The file cache
                will be stored in a sub-directory within this directory (see
                `cache_name`). If `cache_root` is specified and the directory does not
                exist, it is created.
            cache_name: By default, the file cache will be stored in a uniquely-named
                subdirectory of `cache_root` with the prefix ``_filecache_``. Otherwise,
                the file cache will be stored in a subdirectory of `cache_root` called
                ``_filecache_<cache_name>``. Explicitly naming a cache is useful if other
                programs will want to access the same cache, or if you want the directory
                name to be obvious to users browsing the file system. Specifying
                `cache_name` implies that this cache should be persistent on exit.
            delete_on_exit: If True, the cache directory and its contents
                are always deleted on program exit or exit from a :class:`FileCache`
                context manager. If False, the cache is never deleted. By default, an
                unnamed cache (`cache_name` is not specified) will be deleted on exit and
                a named cache will not be deleted on program exit.
            mp_safe: If False, never use multiprocessor-safe locking. If True, always use
                multiprocessor-safe locking. By default, locking is used if `cache_name`
                is specified, as it is assumed that multiple processes will be using the
                named cache simultaneously. If multiple processes will not be using the
                cache simultaneously, a small performance boost can be realized by setting
                `mp_safe` explicitly to False.
            all_anonymous: If True, all accesses to cloud resources will be anonymous, and
                it is not necessary to pass in the `anonymous` option separately to each
                method.
            lock_timeout: The default value for lock timeouts for all methods. This is how
                long to wait, in seconds, if another process is marked as retrieving a
                file before raising an exception. 0 means to not wait at all. A negative
                value means to never time out.
            nthreads: The maximum number of threads to use when doing multiple-file
                retrieval or upload.
            logger: If False, do not do any logging. If None, use the
                global logger set with :func:`set_global_logger`. Otherwise use the
                specified logger.

        Notes:
            :class:`FileCache` can be used as a context, such as::

                with FileCache() as fc:
                    ...

            In this case, the cache directory is created on entry to the context and
            deleted on exit. However, if the cache is marked as shared, the directory will
            not be deleted on exit unless the ``cache_owner=True`` option is used.
        """

        # We try very hard here to make sure that no possible passed-in argument for
        # cache_root or cache_name could result in a directory name that is anything other
        # than a new cache directory. In particular, since we may be deleting this
        # directory later, we want to make sure it's impossible for a bad actor (or just
        # an accidental bad argument) to inject a directory or filename that could result
        # in the deletion of system or user files. One key aspect of this is we do not
        # allow the user to specify the specific subdirectory name without the unique
        # prefix, and we do not allow the directory name to have additional directory
        # components like "..".

        if cache_root is None:
            cache_root = tempfile.gettempdir()
        cache_root = Path(cache_root).expanduser().resolve()
        if not cache_root.exists():
            cache_root.mkdir(parents=True, exist_ok=True)
        if not cache_root.is_dir():
            raise ValueError(f'{cache_root} is not a directory')

        if cache_name is None:
            sub_dir = Path(f'{self._FILE_CACHE_PREFIX}{uuid.uuid4()}')
        elif isinstance(cache_name, str):
            if '/' in cache_name or '\\' in cache_name:
                raise ValueError(
                    f'cache_name argument {cache_name} has directory elements')
            sub_dir = Path(f'{self._FILE_CACHE_PREFIX}{cache_name}')
        else:
            raise TypeError(f'cache_name argument {cache_name} is of improper type')

        is_shared = (cache_name is not None)

        self._delete_on_exit = (delete_on_exit
                                if delete_on_exit is not None else not is_shared)
        self._all_anonymous = all_anonymous
        self._lock_timeout = lock_timeout
        if not isinstance(nthreads, int) or nthreads <= 0:
            raise ValueError(f'nthreads {nthreads} must be a positive integer')
        self._nthreads = nthreads
        self._logger = logger
        self._is_mp_safe = mp_safe if mp_safe is not None else is_shared
        self._upload_counter = 0
        self._download_counter = 0
        self._filecacheprefixes: dict[tuple[str, bool, int], FileCachePrefix] = {}

        self._cache_dir = cache_root / sub_dir
        self._log_debug(f'Creating cache {self._cache_dir}')
        # A non-shared cache (which has a unique name) should never already exist
        self._cache_dir.mkdir(exist_ok=is_shared)

        atexit.register(self._maybe_delete_cache)

    @property
    def cache_dir(self) -> Path:
        """The top-level directory of the cache as a Path object."""
        return self._cache_dir

    @property
    def download_counter(self) -> int:
        """The number of actual file downloads that have taken place."""
        return self._download_counter

    @property
    def upload_counter(self) -> int:
        """The number of actual file uploads that have taken place."""
        return self._upload_counter

    @property
    def all_anonymous(self) -> bool:
        """A bool indicating whether or not to make all cloud accesses anonymous."""
        return self._all_anonymous

    @property
    def lock_timeout(self) -> int:
        """The default timeout in seconds while waiting for a file lock."""
        return self._lock_timeout

    @property
    def nthreads(self) -> int:
        """The default number of threads to use for multiple-file operations."""
        return self._nthreads

    @property
    def delete_on_exit(self) -> bool:
        """A bool indicating whether or not this cache will be deleted on exit."""
        return self._delete_on_exit

    @property
    def is_mp_safe(self) -> bool:
        """A bool indicating whether or not this FileCache is multi-processor safe."""
        return self._is_mp_safe

    def _log_debug(self, msg: str) -> None:
        logger = _GLOBAL_LOGGER if self._logger is None else self._logger
        if logger:
            logger.debug(msg)  # type: ignore

    def _log_error(self, msg: str) -> None:
        logger = _GLOBAL_LOGGER if self._logger is None else self._logger
        if logger:
            logger.error(msg)  # type: ignore

    def _get_source_and_paths(self,
                              full_path: Path | str,
                              anonymous: bool) -> tuple[FileCacheSource, str, Path]:
        if self.all_anonymous:  # Override if all_anonymous was specified
            anonymous = True

        src_str = ''  # Local is the default
        full_path = str(full_path).replace('\\', '/')
        if full_path.startswith(('http://', 'https://', 'gs://', 's3://')):
            # Break 'http://a.b.c.d/e/f' into:
            #   source   'http://a.b.c.d'
            #   sub_path 'e/g'
            idx = full_path.index('//')
            try:
                idx = full_path.index('/', idx+2)
            except ValueError:  # No /
                raise ValueError(f'Invalid path {full_path}')
            src_str = full_path[:idx]
            sub_path = full_path[idx+1:]
        else:
            # Local
            sub_path = str(Path(full_path).expanduser().resolve())
        if not src_str.startswith(('gs://', 's3://')):
            # No such thing as needing credentials for a local file or HTTP
            # so don't overconstrain the source cache
            anonymous = False

        key = (src_str, anonymous)
        if key not in _SOURCE_CACHE:
            if src_str.startswith(('http://', 'https://')):
                _SOURCE_CACHE[key] = FileCacheSourceHTTP(src_str, anonymous=anonymous)
            elif src_str.startswith('gs://'):
                _SOURCE_CACHE[key] = FileCacheSourceGS(src_str, anonymous=anonymous)
            elif src_str.startswith('s3://'):
                _SOURCE_CACHE[key] = FileCacheSourceS3(src_str, anonymous=anonymous)
            else:
                _SOURCE_CACHE[key] = FileCacheSourceLocal(src_str, anonymous=anonymous)

        source = _SOURCE_CACHE[key]
        if source._src_type == 'local':
            local_path = Path(sub_path).expanduser().resolve()
        else:
            local_path = self._cache_dir / source._cache_subdir / sub_path

        return source, sub_path, local_path

    def _lock_path(self, path: Path | str) -> Path:
        path = Path(path)
        return path.parent / f'{self._LOCK_PREFIX}{path.name}'

    def exists(self,
               full_path: str,
               anonymous: bool = False) -> bool:
        """Check if a file exists without downloading it.

        Parameters:
            full_path: The full path of the file (including any source prefix).
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.

        Returns:
            True if the file exists. Note that it is possible that a file could exist and
            still not be downloadable due to permissions. False if the file does not
            exist. This includes bad bucket or webserver names, lack of permission to
            examine a bucket's contents, etc.

        Raises:
            ValueError: If the path is invalidly constructed.
        """

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)
        if source._src_type != 'local' and local_path.is_file():
            # Already in the cache
            return True

        self._log_debug(f'Checking file for existence: {full_path}')

        ret = source.exists(sub_path)

        if ret:
            self._log_debug(f'  File exists: {full_path}')
        else:
            self._log_debug(f'  File does not exist: {full_path}')

        return ret

    def get_local_path(self,
                       full_path: str,
                       anonymous: bool = False,  # XXX This should be removed
                       create_parents: bool = True) -> Path:
        """Return the local path for the given full_path.

        Parameters:
            full_path: The full path of the file (including any source prefix).
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.
            create_parents: If True, create all parent directories. This
                is useful when getting the local path of a file that will be uploaded.

        Returns:
            The Path of the filename in the temporary directory, or the `full_path` if the
            file source is local. The file does not have to exist because this path could
            be used for writing a file to upload. To facilitate this, a side effect of
            this call (if `create_parents` is True) is that the complete parent directory
            structure will be created by this function as necessary.
        """

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)
        if create_parents:
            local_path.parent.mkdir(parents=True, exist_ok=True)

        if source._src_type == 'local':
            self._log_debug(f'Returning local path for {full_path} (local file)')
        else:
            self._log_debug(f'Returning local path for {full_path} as {local_path}')

        return local_path

    def retrieve(self,
                 full_path: str | list[str] | tuple[str],
                 anonymous: bool = False,
                 lock_timeout: Optional[int] = None,
                 nthreads: Optional[int] = None,
                 exception_on_fail: bool = True,
                 **kwargs) -> Path | Exception | list[Path | Exception]:
        """Retrieve file(s) from the given location(s) and store in the file cache.

        Parameters:
            full_path: The full path of the file, including any source prefix. If
                `full_path` is a list or tuple, all full paths are retrieved. This may be
                more efficient because files can be downloaded in parallel. It is OK to
                retrieve files from multiple sources using one call.
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.
            lock_timeout: How long to wait, in seconds, if another process is marked as
                retrieving the file before raising an exception. 0 means to not wait at
                all. A negative value means to never time out. None means to use the
                default value for this :class:`FileCache` instance.
            nthreads: The maximum number of threads to use when doing multiple-file
                retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
            exception_on_fail: If True, if any file does not exist or download fails a
                FileNotFound exception is raised, and if any attempt to acquire a lock or
                wait for another process to download a file fails a TimeoutError is
                raised. If False, the function returns normally and any failed download is
                marked with the Exception that caused the failure in place of the returned
                Path.

        Returns:
            The Path of the filename in the temporary directory (or the original full path
            if local). If `full_path` was a list or tuple of paths, then instead return a
            list of Paths of the filenames in the temporary directory (or the original
            full path if local). If `exception_on_fail` is False, any Path may be an
            Exception if that file does not exist or the download failed or a timeout
            occurred.

        Raises:
            FileNotFoundError: If a file does not exist or could not be downloaded, and
                exception_on_fail is True.
            TimeoutError: If we could not acquire the lock to allow downloading of a file
                within the given timeout or, for a multi-file download, if we timed out
                waiting for other processes to download locked files, and
                exception_on_fail is True.

        Notes:
            File download is normally an atomic operation; a program will never see a
            partially-downloaded file, and if a download is interrupted there will be no
            file present. However, when downloading multiple files at the same time, as
            many files as possible are downloaded before an exception is raised.
        """

        if lock_timeout is None:
            lock_timeout = self.lock_timeout
        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        # Technically we could just do everything as a locked multi-download, but we
        # separate out the cases for efficiency
        if isinstance(full_path, (list, tuple)):
            if nthreads is None:
                nthreads = self.nthreads
            sources = []
            sub_paths = []
            local_paths = []
            for path in full_path:
                source, sub_path, local_path = self._get_source_and_paths(path, anonymous)
                sources.append(source)
                sub_paths.append(sub_path)
                local_paths.append(local_path)
            if self.is_mp_safe:
                return self._retrieve_multi_locked(sources, sub_paths, local_paths,
                                                   lock_timeout, nthreads,
                                                   exception_on_fail)
            return self._retrieve_multi_unlocked(sources, sub_paths, local_paths,
                                                 nthreads, exception_on_fail)

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)

        if source._src_type == 'local':
            self._log_debug(f'Accessing local file {sub_path}')
            try:
                return source.retrieve(sub_path, local_path)
            except Exception as e:
                if exception_on_fail:
                    raise
                return e

        # If the file actually exists, it's always safe to return it
        if local_path.is_file():
            self._log_debug(f'Accessing cached file for {full_path} at {local_path}')
            return local_path

        if self.is_mp_safe:
            return self._retrieve_locked(source, sub_path, local_path, lock_timeout,
                                         exception_on_fail)
        return self._retrieve_unlocked(source, sub_path, local_path, exception_on_fail)

    def _retrieve_unlocked(self,
                           source: FileCacheSource,
                           sub_path: str,
                           local_path: Path,
                           exception_on_fail: bool) -> Path | Exception:
        """Retrieve a single file from the storage location without lock protection."""

        self._log_debug(f'Downloading {source._src_prefix_}{sub_path} into {local_path}')
        try:
            ret = source.retrieve(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise
            return e

        self._download_counter += 1

        return ret

    def _retrieve_locked(self,
                         source: FileCacheSource,
                         sub_path: str,
                         local_path: Path,
                         lock_timeout: int,
                         exception_on_fail: bool) -> Path | Exception:
        """Retrieve a single file from the storage location with lock protection."""

        lock_path = self._lock_path(local_path)
        lock = filelock.FileLock(lock_path, timeout=lock_timeout)
        try:
            lock.acquire()
        except filelock._error.Timeout as e:
            if exception_on_fail:
                raise
            return e
        self._log_debug(
            f'Downloading {source._src_prefix_}{sub_path} into {local_path} with locking')
        try:
            ret = source.retrieve(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise
            return e
        finally:
            # There is a potential race condition here in the case of a raised
            # exception, because after we release the lock but before we delete
            # it, someone else could notice the file isn't downloaded and lock
            # it for another download attempt, and then we would delete someone
            # else's lock (because on Linux locks are only advisory). However,
            # we have to do it in this order because otherwise it won't work on
            # Windows, where locks are not just advisory. However, the worst
            # that could happen is we end up attempting to download the file
            # twice.
            lock.release()
            lock_path.unlink(missing_ok=True)

        self._download_counter += 1

        return ret

    def _retrieve_multi_unlocked(self,
                                 sources: list[FileCacheSource],
                                 sub_paths: list[str],
                                 local_paths: list[Path],
                                 nthreads: int,
                                 exception_on_fail: bool) -> list[Path | Exception]:
        """Retrieve multiple files from storage locations without lock protection."""

        # Return Paths (or Exceptions) in the same order as sub_paths
        func_ret: list[Path | Exception | None] = [None] * len(sources)

        files_not_exist = []

        source_dict: dict[str, list[tuple[int, FileCacheSource, str, Path]]] = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing multi-file retrieval of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            if source._src_type == 'local':
                self._log_debug(f'    Local file   {pfx}{sub_path}')
                try:
                    func_ret[idx] = source.retrieve(sub_path, local_path)
                except Exception as e:
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            if local_path.is_file():
                self._log_debug(f'    Cached file  {pfx}{sub_path} at {local_path}')
                func_ret[idx] = local_path
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))
            self._log_debug(f'    To download {pfx}{sub_path}')

        # Now go through the sources, package up the paths to retrieve, and retrieve
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            source_idxes, _, source_sub_paths, source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'  Performing multi-file download for prefix {source_pfx}:')
            for sub_path in source_sub_paths:
                self._log_debug(f'    {sub_path}')
            rets = source.retrieve_multi(source_sub_paths, source_local_paths,
                                         nthreads=nthreads)
            assert len(source_idxes) == len(rets)
            for ret, sub_path in zip(rets, source_sub_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Download failed: {sub_path} {ret}')
                    files_not_exist.append(f'{source_pfx}{sub_path}')
                else:
                    self._download_counter += 1

            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        if files_not_exist:
            self._log_debug('Multi-file retrieval completed with errors')
            if exception_on_fail:
                exc_str = f"File(s) do not exist: {', '.join(files_not_exist)}"
                raise FileNotFoundError(exc_str)
        else:
            self._log_debug('Multi-file retrieval complete')

        return cast(list[Path | Exception], func_ret)

    def _retrieve_multi_locked(self,
                               sources: list[FileCacheSource],
                               sub_paths: list[str],
                               local_paths: list[Path],
                               lock_timeout: int,
                               nthreads: int,
                               exception_on_fail: bool) -> list[Path | Exception]:
        """Retrieve multiple files from storage locations with lock protection."""

        start_time = time.time()

        # Return Paths (or Exceptions) in the same order as sub_paths
        func_ret: list[Path | Exception | None] = [None] * len(sources)

        files_not_exist = []

        wait_to_appear = []  # Locked by another process (they are downloading it)

        source_dict: dict[str, list[tuple[int, FileCacheSource, str, Path]]] = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing locked multi-file retrieval of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            # No need to lock for local files
            if source._src_type == 'local':
                self._log_debug(f'    Local file   {pfx}{sub_path}')
                try:
                    func_ret[idx] = source.retrieve(sub_path, local_path)
                except Exception as e:
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            # Since all download operations for individual files are atomic, no need to
            # lock if the file actually exists
            if local_path.is_file():
                self._log_debug(f'    Cached file  {pfx}{sub_path} at {local_path}')
                func_ret[idx] = local_path
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))
            self._log_debug(f'    To download {pfx}{sub_path}')

        # Now go through the sources, package up the paths to retrieve, and retrieve
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            orig_source_idxes, _, orig_source_sub_paths, orig_source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'  Performing locked multi-file download for prefix {source_pfx}:')
            for sub_path in orig_source_sub_paths:
                self._log_debug(f'      {sub_path}')

            # We first loop through the local paths and try to acquire locks on all
            # the files. If we fail to get a lock on a file, it must be downloading
            # somewhere else, so we just remove it from the list of files to download
            # right now and then wait for it to appear later.
            lock_list = []
            source_idxes = []
            source_sub_paths = []
            source_local_paths = []
            for idx, sub_path, local_path in zip(orig_source_idxes,
                                                 orig_source_sub_paths,
                                                 orig_source_local_paths):
                lock_path = self._lock_path(local_path)
                # We don't actually want to wait for a lock to clear, we just want
                # to know if someone else is downloading the file right now
                lock = filelock.FileLock(lock_path, timeout=0)
                try:
                    lock.acquire()
                except filelock._error.Timeout:
                    self._log_debug(f'    Failed to lock: {sub_path}')
                    wait_to_appear.append((idx, f'{source_pfx}{sub_path}', local_path,
                                           lock_path))
                    continue
                lock_list.append((lock_path, lock))
                source_idxes.append(idx)
                source_sub_paths.append(sub_path)
                source_local_paths.append(local_path)

            # Now we can actually download the files that we locked
            rets = source.retrieve_multi(source_sub_paths, source_local_paths,
                                         nthreads=nthreads)
            assert len(source_sub_paths) == len(rets)
            for ret, sub_path in zip(rets, source_sub_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Download failed: {sub_path} {ret}')
                    files_not_exist.append(f'{source_pfx}{sub_path}')
                else:
                    self._log_debug(f'    Successfully downloaded: {sub_path}')
                    self._download_counter += 1

            # Release all the locks
            for lock_path, lock in lock_list:
                # There is a potential race condition here in the case of a raised
                # exception, because after we release the lock but before we delete
                # it, someone else could notice the file isn't downloaded and lock
                # it for another download attempt, and then we would delete someone
                # else's lock (because on Linux locks are only advisory). However,
                # we have to do it in this order because otherwise it won't work on
                # Windows, where locks are not just advisory. However, the worst
                # that could happen is we end up attempting to download the file
                # twice.
                lock.release()
                lock_path.unlink(missing_ok=True)

            # Record the results
            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        # If wait_to_appear is not empty, then we failed to acquire at least one lock,
        # which means that another process was downloading the file. So now we just
        # sit here and wait for all of the missing files to magically show up, or for
        # us to time out. If the lock file disappears but the destination file isn't
        # present, that means the other process failed in its download.
        timed_out = False
        while wait_to_appear:
            new_wait_to_appear = []
            for idx, full_path, local_path, lock_path in wait_to_appear:
                if local_path.is_file():
                    func_ret[idx] = local_path
                    self._log_debug(f'  Downloaded elsewhere: {full_path}')
                    continue
                if not lock_path.is_file():
                    func_ret[idx] = FileNotFoundError(
                        f'Another process failed to download {full_path}')
                    self._log_debug(f'  Download elsewhere failed: {full_path}')
                    continue
                new_wait_to_appear.append((idx, full_path, local_path, lock_path))

            if not new_wait_to_appear:
                break

            wait_to_appear = new_wait_to_appear
            if time.time() - start_time > lock_timeout:
                exc = TimeoutError(
                    'Timeout while waiting for another process to finish downloading')
                self._log_debug(
                    '  Timeout while waiting for another process to finish downloading:')
                for idx, full_path, local_path, lock_path in wait_to_appear:
                    func_ret[idx] = exc
                    self._log_debug(f'    {full_path}')
                if exception_on_fail:
                    raise exc
                timed_out = True
                break
            time.sleep(1)  # Wait 1 second before trying again

        if files_not_exist or timed_out:
            self._log_debug('Multi-file retrieval completed with errors')
            if exception_on_fail and files_not_exist:
                exc_str = f"File(s) do not exist: {', '.join(files_not_exist)}"
                raise FileNotFoundError(exc_str)
        else:
            self._log_debug('Multi-file retrieval complete')

        return cast(list[Path | Exception], func_ret)

    def upload(self,
               full_path: str | list[str] | tuple[str],
               anonymous: bool = False,
               nthreads: Optional[int] = None,
               exception_on_fail: bool = True) -> Path | Exception | list[Path |
                                                                          Exception]:
        """Upload file(s) from the file cache to the storage location(s).

        Parameters:
            full_path: The full path of the file, including any source prefix. If
                `full_path` is a list or tuple, the complete list of files is uploaded.
                This may be more efficient because files can be uploaded in parallel.
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.
            nthreads: The maximum number of threads to use when doing multiple-file
                retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
            exception_on_fail: If True, if any file does not exist or upload fails an
                exception is raised. If False, the function returns normally and any
                failed upload is marked with the Exception that caused the failure in
                place of the returned path.

        Returns:
            The Path of the filename in the cache directory (or the original full path
            if local). If `full_path` was a list or tuple of paths, then instead return a
            list of Paths of the filenames in the temporary directory (or the original
            full path if local). If `exception_on_fail` is False, any Path may be an
            Exception if that file does not exist or the upload failed.

        Raises:
            FileNotFoundError: If a file to upload does not exist or the upload failed,
                and exception_on_fail is True.
        """

        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        if isinstance(full_path, (list, tuple)):
            if nthreads is None:
                nthreads = self.nthreads
            sources = []
            sub_paths = []
            local_paths = []
            for path in full_path:
                source, sub_path, local_path = self._get_source_and_paths(path, anonymous)
                sources.append(source)
                sub_paths.append(sub_path)
                local_paths.append(local_path)
            return self._upload_multi(sources, sub_paths, local_paths, nthreads,
                                      exception_on_fail)

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)

        if source._src_type == 'local':
            self._log_debug(f'Uploading {local_path} (local file)')
        else:
            self._log_debug(f'Uploading {local_path} to {source._src_prefix_}{sub_path}')

        try:
            ret = source.upload(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise e
            else:
                return e

        self._upload_counter += 1

        return ret

    def _upload_multi(self,
                      sources: list[FileCacheSource],
                      sub_paths: list[str],
                      local_paths: list[Path],
                      nthreads: int,
                      exception_on_fail: bool) -> list[Path | Exception]:
        """Upload multiple files to storage locations."""

        func_ret: list[Path | Exception | None] = [None] * len(sources)

        files_not_exist = []
        files_failed = []

        source_dict: dict[str, list[tuple[int, FileCacheSource, str, Path]]] = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing multi-file upload of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            if source._src_type == 'local':
                try:
                    func_ret[idx] = source.upload(sub_path, local_path)
                    self._log_debug(f'  Local file     {pfx}{sub_path}')
                except FileNotFoundError as e:
                    self._log_debug(f'  LOCAL FILE DOES NOT EXIST {pfx}{sub_path}')
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            if not Path(local_path).is_file():
                self._log_debug(f'  LOCAL FILE DOES NOT EXIST {pfx}{sub_path}')
                files_not_exist.append(sub_path)
                func_ret[idx] = FileNotFoundError(
                    f'File does not exist: {pfx}{sub_path}')
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))

        # Now go through the sources, package up the paths to upload, and upload
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            source_idxes, _, source_sub_paths, source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'Performing multi-file upload for prefix {source_pfx}:')
            for sub_path in source_sub_paths:
                self._log_debug(f'  {sub_path}')
            rets = source.upload_multi(source_sub_paths, source_local_paths,
                                       nthreads=nthreads)
            assert len(source_idxes) == len(rets)
            for ret, local_path in zip(rets, source_local_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Upload failed: {sub_path} {ret}')
                    files_failed.append(str(local_path))
                else:
                    self._upload_counter += 1

            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        if exception_on_fail:
            exc_str = ''
            if files_not_exist:
                exc_str += f"File(s) do not exist: {', '.join(files_not_exist)}"
            if files_failed:
                if exc_str:
                    exc_str += ' AND '
                exc_str += f"Failed to upload file(s): {', '.join(files_failed)}"
            if exc_str:
                raise FileNotFoundError(exc_str)

        return cast(list[Path | Exception], func_ret)

    @contextlib.contextmanager
    def open(self,
             full_path: str,
             mode: str = 'r',
             *args,
             anonymous: bool = False,
             lock_timeout: Optional[int] = None,
             **kwargs):
        """Retrieve+open or open+upload a file as a context manager.

        If `mode` is a read mode (like ``'r'`` or ``'rb'``) then the file will be first
        retrieved by calling :meth:`retrieve` and then opened. If the `mode` is a write
        mode (like ``'w'`` or ``'wb'``) then the file will be first opened for write, and
        when this context manager is exited the file will be uploaded.

        Parameters:
            full_path: The filename to open.
            mode: The mode string as you would specify to Python's `open()` function.
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.
            lock_timeout: How long to wait, in seconds, if another process is marked as
                retrieving the file before raising an exception. 0 means to not wait at
                all. A negative value means to never time out. If None, use the default
                value for this :class:`FileCache` instance.

        Returns:
            The same object as would be returned by the normal `open()` function.
        """

        if mode[0] == 'r':
            local_path = self.retrieve(full_path,
                                       anonymous=anonymous, lock_timeout=lock_timeout)
            with open(cast(Path, local_path), mode, *args, **kwargs) as fp:
                yield fp
        else:  # 'w', 'x', 'a'
            local_path = self.get_local_path(full_path, anonymous=anonymous)
            with open(local_path, mode, *args, **kwargs) as fp:
                yield fp
            self.upload(full_path, anonymous=anonymous)

    def new_prefix(self,
                   prefix: str,
                   anonymous: bool = False,
                   lock_timeout: Optional[int] = None,
                   nthreads: Optional[int] = None,
                   **kwargs) -> FileCachePrefix:
        """Create a new FileCachePrefix with the given prefix.

        Parameters:
            prefix: The prefix for the storage location, which may include the source
                prefix was well as any top-level directories. All accesses made through
                this :class:`FileCachePrefix` instance will have this prefix prepended to
                their file path.
            anonymous: If True, access cloud resources (GS and S3) without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment. This parameter can be overridden by the :meth:`__init__`
                `all_anonymous` argument.
            lock_timeout: How long to wait, in seconds, if another process is marked as
                retrieving the file before raising an exception. 0 means to not wait at
                all. A negative value means to never time out. None means to use the
                default value for this :class:`FileCache` instance.
            nthreads: The maximum number of threads to use when doing multiple-file
                retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
        """

        if isinstance(prefix, Path):
            prefix = str(prefix)
        if not isinstance(prefix, str):
            raise TypeError('prefix is not a str or Path')
        if not prefix.startswith(('http://', 'https://', 'gs://', 's3://')):
            prefix = prefix.replace('\\', '/').rstrip('/')
        if lock_timeout is None:
            lock_timeout = self.lock_timeout
        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')
        if nthreads is None:
            nthreads = self.nthreads
        key = (prefix, anonymous, lock_timeout)
        if key not in self._filecacheprefixes:
            self._filecacheprefixes[key] = FileCachePrefix(
                prefix, self, anonymous=anonymous, lock_timeout=lock_timeout,
                nthreads=nthreads, **kwargs)
        return self._filecacheprefixes[key]

    def _maybe_delete_cache(self):
        """Delete this cache if delete_on_exit is True."""

        if self._delete_on_exit:
            self.delete_cache()

    def delete_cache(self):
        """Delete all files stored in the cache including the cache directory.

        Notes:
            It is permissible to call :meth:`delete_cache` more than once. It is also
            permissible to call :meth:`delete_cache`, then perform more operations that
            place files in the cache, then call :meth:`delete_cache` again.
        """

        self._log_debug(f'Deleting cache {self._cache_dir}')

        # Verify this is really a cache directory before walking it and deleting
        # every file. We are just being paranoid to make sure this never does a
        # "rm -rf" on a real directory like "/".
        if not Path(self._cache_dir).name.startswith(self._FILE_CACHE_PREFIX):
            raise ValueError(
                f'Cache directory does not start with {self._FILE_CACHE_PREFIX}')

        # Delete all of the files and subdirectories we left behind, including the
        # file cache directory itself.
        # We would like to use Path.walk() but that was only added in Python 3.12. We
        # allow remove and rmdir to fail with FileNotFoundError because we could have two
        # programs deleting a shared cache at the same time fighting each other, or
        # someone could have asked for the local path to a file and then never written
        # anything there.
        for root, dirs, files in os.walk(self._cache_dir, topdown=False):
            for name in files:
                if name.startswith(self._LOCK_PREFIX):
                    self._log_error(
                        'Deleting cache that has an active lock file: {root}/{name}')
                self._log_debug(f'  Removing file {root}/{name}')
                try:
                    os.remove(os.path.join(root, name))
                except FileNotFoundError:  # pragma: no cover - race condition only
                    pass
            for name in dirs:
                self._log_debug(f'  Removing dir {root}/{name}')
                try:
                    os.rmdir(os.path.join(root, name))
                except FileNotFoundError:  # pragma: no cover - race condition only
                    pass

        self._log_debug(f'  Removing dir {self._cache_dir}')
        try:
            os.rmdir(self._cache_dir)
        except FileNotFoundError:  # pragma: no cover - race condition only
            pass

    def __enter__(self):
        """Enter the context manager for creating a FileCache."""
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Exit the context manage for a FileCache, executing any cache deletion."""
        self._maybe_delete_cache()
        # Since the cache is deleted, no need to delete it again later
        atexit.unregister(self._maybe_delete_cache)
